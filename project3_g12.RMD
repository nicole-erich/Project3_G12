---
title: "project3_g12"
author: "Nicole Erich, Amanda Coker, Anying (Ann) Li, Liuxuan (Kelly) Yu"
date: "March 27, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(knitr)
library(pander)
library(ggplot2)
```

### Question 1
#### Gurobi Access

All teammates have access to Gurobi through RStudio.

```{r echo=FALSE}
# ######################
# Windows install gurobi
# ######################

#install.packages('slam')
#install.packages('c:\\gurobi702\\win64\\R\\gurobi_7.0-2.zip', repos=NULL)

# Nicole Set directory and load
#setwd("~/5 - MSBA/2017 01 Optimization/Group Projects/Project 3")
load("data.rdata")

# ######################
# Mac install gurobi
# ######################

# load("~/Downloads/data.rdata")
# install.packages('/Library/gurobi702/mac64/R/gurobi_7.0-2.tgz', repos=NULL,type="source")
# change the directory if you are not using MAX OS
# install.packages("glmnet", repos = "http://cran.us.r-project.org")
```

### Question 2
#### Apply both MIQP and Lasso to the given data

#### Lasso:

```{r echo=FALSE, warnings=FALSE, message=FALSE}
# Indirect Feature Selection (Lasso Regression)
library(glmnet)
fit = glmnet(X, y, alpha = 1)
plot(fit, xvar = "lambda", label = TRUE,main="Lasso Regression\n")

# The plot shows a cut-off at Log Lambda = -2; we will use e^(-2) as the penalty term of our Lasso model
lasso_beta = coef(fit,s=exp(-2))[-1]
```

Because the plot shows a cut-off at log($\lambda$) = -2, we will use $e^{-2}$ as the penalty term for the Lasso model.

We can inspect the remaining non-zero coefficients:

```{r echo=FALSE}
lasso_non_zero = as.matrix(cbind(match(lasso_beta[lasso_beta>0],lasso_beta),lasso_beta[lasso_beta>0]))
colnames(lasso_non_zero) = c("Coef. Number","lasso_beta")

# Formatted Table
set.alignment('right',row.names='left')
pander(lasso_non_zero)
```


Comparing these to the original coefficients, we check to see if Lasso zeroed any "real" coefficients, or if it added any that were non-siginifcant:

```{r echo=FALSE}
lasso_real_comp = as.matrix(cbind(lasso_beta,beta_real))

# Remove rows that are both 0
lasso_real_comp = as.matrix(subset(lasso_real_comp,subset=lasso_beta>0 | beta_real>0))

# Formatted Table
set.alignment('right',row.names='left')
pander(round(lasso_real_comp,4))
```

As we can see, the Lasso regression actually did keep one coefficient that was zero in the original betas, but its value is 0.009661, which is nearly zero. Overall, the Lasso seems to have done fairly well in estimating the true betas. 


#### MIQP:

```{r echo=FALSE}
# Direct Selection (MIP)
construct_MIQP = function(X,y,k,M){
  # this function is to formulate the Mixed Interger Programming Problem
  n = dim(X)[2]
  # n = 64 in our case
  
  # we created 128 variables in total:
  # the first 64 (B1, B2,.., B64) are continuous variables representing each independent variable; 
  # the last 64 (Z1, Z2,.., Z64) are binary variables: Zi (i in 1:64) indicates whether Bi is 0
  model <- list()
  model$vtype <- c(rep('C',n),rep('B',n))
  
  # Formulate Constraints
  A = matrix(0,2*n,2*n)
  # 1. for -M*Zi <= Bi <= M*Zi:
  A[1:n,1:n] = -1*diag(n)
  A[1:n,(n+1):(2*n)] = -M*diag(n)
  A[(n+1):(2*n),1:n] = diag(n)
  A[(n+1):(2*n),(n+1):(2*n)] = -M*diag(n)
  # 2. for Z1 + Z2 + ... + Z64 = k:
  A1 = c(rep(0,n),rep(1,n))
  model$A <- rbind(A1,A)
  model$sense <- rep("<=",2*n+1)
  model$rhs <- c(k,rep(0,2*n))
  
  # Formulate Objective (i.e. sum of squared residuals in our case)
  Q = matrix(0,2*n,2*n)
  # 1. quadratic component of the objective function
  Q[0:n,0:n] = t(X) %*% X
  model$Q = Q
  # 2. linear component of the objective function
  model$obj <- c(-2*y %*% X,rep(0,n))
  
  result <- gurobi(model, list(ResultFile='model.mps',OutputFlag=0))
  return(result)
}
```

```{r include=FALSE}
library(gurobi)

# Set up initial run of MIQP
k=8
M = 0.1
initial_sol = construct_MIQP(X,y,k,M)
max = max(abs(initial_sol$x[1:64]))
```


Using the k value of 8 and an initial value of M as 0.1, we calculate the maximum M value (`r max`) from an initial run of the MIQP. We will continue doubling M and finding the max until M reaches the max value. The results of these iterations are below.

```{r echo=FALSE}
# Iterations of M
while (M<=max){
  M = M*2
  current_sol = construct_MIQP(X,y,k,M)
  max = max(abs(current_sol$x[1:64]))
  print(M)
}
```

Using the solution obtained with the max M iteration above, the MIQP model keeps the following coefficients. 

```{r echo=FALSE}
MIQP_beta = current_sol$x[1:64]
sol_compare = as.matrix(cbind(lasso_beta,MIQP_beta,beta_real))

MIQP_non_zero = as.matrix(cbind(match(MIQP_beta[MIQP_beta>0],MIQP_beta),MIQP_beta[MIQP_beta>0]))
colnames(MIQP_non_zero) = c("Coef. Number","MIQP_beta")

# Formatted Table
set.alignment('right',row.names='left')
pander(MIQP_non_zero)
```

Comparing these to the original coefficients, we check to see if MIQP zeroed any "real" coefficients, or if it added any that were non-siginifcant:

```{r echo=FALSE}
MIQP_real_comp = as.matrix(cbind(MIQP_beta,beta_real))

# Remove rows that are both 0
MIQP_real_comp = as.matrix(subset(MIQP_real_comp,subset=MIQP_beta>0 | beta_real>0))

# Formatted Table
set.alignment('right',row.names='left')
pander(round(MIQP_real_comp,4))
```

In this case, the MIQP found all of the same coefficents as the original betas without missing any. 

### Question 3
#### Prediction Error
The next step is to calculate the actual errors to find which model performed best in estimating the coefficients.

```{r echo=FALSE}
# Create function to compute the error for each method
compute_error = function(sol_beta, real_beta, X){
  error = X%*%sol_beta - X%*%real_beta
  return(sum(error^2)/sum((X%*%real_beta)^2))
}

# Compute error for Lasso, MIQP, and original
lasso_error = compute_error(lasso_beta, beta_real, X)
MIQP_error = compute_error(MIQP_beta, beta_real, X)
real_error = compute_error(beta_real, beta_real, X)
```

To test the error function, if we compare the real betas against themselves, we would expect for the error to be 0. Running through the function, we obtain `r real_error`, so the function is working according to plan.

The Lasso error is `r round(lasso_error,4)` and the MIQP error is `r round(MIQP_error,4)`. Based on this calculation, we can determine that MIQP performed better in predicting the actual betas.